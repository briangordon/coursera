#!/usr/bin/env python
"""
For downloading video lectures from Coursera classes. Given a class name, it scrapes the course listing page to get the section (week) and lecture names, and then downloads the lectures into appropriately named files and directories.

Examples:
  coursera-dl saas
  coursera-dl -l listing.html -o saas --skip-download

Author: 
  John Lehmann (first last at geemail dotcom or @jplehmann)
"""

import sys, os, re, string
import urllib2
import tempfile
import subprocess
import argparse
import StringIO
from BeautifulSoup import BeautifulSoup        

def lecture_video_url(leclink):
  lecpage = get_page(leclink)
  soup = BeautifulSoup(lecpage)
  return str(soup.find('source')["src"])

def get_syllabus_url(className):
  """Return the Coursera index/syllabus URL."""
  return "http://class.coursera.org/%s/lecture/preview" % className

def get_page(url):
  """Download an HTML page"""
  return urllib2.urlopen(url).read()

def get_syllabus(class_name, local_page=False):
  """ Get the course listing webpage."""
  if (not (local_page and os.path.exists(local_page))):
    url = get_syllabus_url(class_name)
    page = get_page(url)
    print "Downloaded %s (%d bytes)" % (url, len(page))
    # cache the page if we're in 'local' mode
    if (local_page):
      open(local_page, 'w').write(page)
  else:
    page = open(local_page).read()
  return page

def clean_filename(s):
  """Sanitize a string to be used as a filename."""
  # strip paren portions which contain trailing time length (...)
  s = re.sub("\([^\(]*$", "", s)
  s = s.strip().replace(':','-').replace(' ', '_')
  valid_chars = "-_.()%s%s" % (string.ascii_letters, string.digits)
  return ''.join(c for c in s if c in valid_chars)

def parse_syllabus(page):
  """Parses a Coursera course listing/syllabus page. 
  Each section is a week of classes."""
  sections = []
  soup = BeautifulSoup(page)
  # traverse sections
  for stag in soup.findAll(attrs={'class':'list_header'}):
    assert stag.string != None, "couldn't find section"
    section_name = clean_filename(stag.string)
    print section_name
    lectures = [] # resources for 1 lecture
    # traverse resources (e.g., video, ppt, ..)
    for vtag in stag.parent.nextSibling.findAll('li'):
      assert vtag.a.contents[0], "couldn't get lecture name"
      vname = clean_filename(vtag.a.contents[0])
      print "  ", vname
      leclink = vtag.find('a')['href']
      lecture = lecture_video_url(leclink)
      lectures.append((vname, lecture))
    sections.append((section_name, lectures))
  print "Found %d sections and %d lectures on this page" % \
    (len(sections), sum((len(s[1]) for s in sections)))
  if (not len(sections)):
    print "Probably wrong class name"
  return sections

def download_lectures(
  wget_bin, 
  class_name, 
  sections, 
  overwrite=False, 
  skip_download=False, 
  section_filter=None, 
  lecture_filter=None
  ):
  """Downloads lecture resources described by sections."""

  def format_section(num, section):
    return "%s_%02d_%s" % (class_name.upper(), num, section)

  def format_resource(num, name, format):
    return "%02d_%s.%s" % (num, name, format)

  for (secnum, (section, lectures)) in enumerate(sections):
    if section_filter and not re.search(section_filter, section): 
      #print "Skipping b/c of sf: ", section_filter, section
      continue
    sec = format_section(secnum+1, section)
    for (lecnum, (lecname, lecture)) in enumerate(lectures):
      if lecture_filter and not re.search(lecture_filter, lecname): 
        continue
      if not os.path.exists(sec):
        os.mkdir(sec)
      # write lecture resource
      url = lecture
      lecfn = os.path.join(sec, format_resource(lecnum+1, lecname, "mp4"))
      print lecfn
      if overwrite or not os.path.exists(lecfn):
        if not skip_download: 
          download_file(url, lecfn, wget_bin)
        else: 
          open(lecfn, 'w').close()  # touch

def download_file(url, fn, wget_bin):
  """Downloads file and removes current file if aborted by user."""
  try:
    if wget_bin:
      download_file_wget(wget_bin, url, fn)
    else:
      download_file_nowget(url, fn)
  except KeyboardInterrupt, e: 
    print "\nKeyboard Interrupt -- Removing partial file:", fn
    os.remove(fn)
    sys.exit()

def download_file_wget(wget_bin, url, fn):
  """Downloads a file using wget.  Could possibly use python to stream files to
  disk, but wget is robust and gives nice visual feedback."""
  cmd = [wget_bin, url, "-O", fn, "--no-check-certificate"]
  print "Executing wget:", cmd 
  retcode = subprocess.call(cmd)

def download_file_nowget(url, fn):
  """'Native' python downloader -- slower than wget."""
  print "Downloading %s -> %s" % (url, fn)
  urlfile = urllib2.urlopen(url)
  chunk_sz = 1048576
  bytesread = 0
  f = open(fn, "wb")
  while True:
    data = urlfile.read(chunk_sz)
    if not data:
      print "."
      break
    f.write(data)
    bytesread += len(data)
    print "\r%d bytes read" % bytesread,
    sys.stdout.flush()

def parseArgs():
  parser = argparse.ArgumentParser(description='Download Coursera.org lecture material and resources.')
  # positional
  parser.add_argument('class_name', action='store', 
    help='name of the class (e.g. "nlp")')
  # optional
  parser.add_argument('-sf', '--section_filter', dest='section_filter', 
    action='store', default=None, help='only download sections which contain this regex (default: disabled)')
  parser.add_argument('-lf', '--lecture_filter', dest='lecture_filter', 
    action='store', default=None, help='only download lectures which contain this regex (default: disabled)')
  parser.add_argument('-w', '--wget_bin', dest='wget_bin', 
    action='store', default=None, help='wget binary if it should be used for downloading')
  parser.add_argument('-o', '--overwrite', dest='overwrite', 
    action='store_true', default=False, 
    help='whether existing files should be overwritten (default: False)')
  parser.add_argument('-l', '--process_local_page', dest='local_page', 
    help='for debugging: uses or creates local cached version of syllabus page')
  parser.add_argument('--skip-download', dest='skip_download', 
    action='store_true', default=False, 
    help='for debugging: skip actual downloading of files')
  args = parser.parse_args()
  return args

def main():
  args = parseArgs()
  page = get_syllabus(args.class_name, args.local_page)
  sections = parse_syllabus(page)
  download_lectures(
    args.wget_bin, 
    args.class_name, 
    sections, 
    args.overwrite, 
    args.skip_download, 
    args.section_filter, 
    args.lecture_filter
  )

if __name__ == "__main__":
  main()
